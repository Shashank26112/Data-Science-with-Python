# -*- coding: utf-8 -*-
"""BikeSharing LR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q95ZeK93cckoC6Fo2kJGUtDvRbAzvi3p
"""

# Demand for shared bikes
# Before covid data shared and we want to tell the business
# what could be the features that will help them boost their revenue after lockdown is removed


import numpy as np , pandas as pd
import matplotlib.pyplot as plt, seaborn as sns

"""### **Overview**


### **Problem Statement**

To build a multiple linear regression model for the prediction of demand for shared bikes.

A bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a "dock" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.

A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state.

In such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.

They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:

1. Which variables are significant in predicting the demand for shared bikes.
2. How well those variables describe the bike demands
Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors.

Business Goal:
You are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market.

=========================================
Dataset characteristics
=========================================
day.csv have the following fields:

	- instant: record index
	- dteday : date
	- season : season (1:spring, 2:summer, 3:fall, 4:winter)
	- yr : year (0: 2018, 1:2019)
	- mnth : month ( 1 to 12)
	- holiday : weather day is a holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)
	- weekday : day of the week
	- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.
	+ weathersit :
		- 1: Clear, Few clouds, Partly cloudy, Partly cloudy
		- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
		- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
		- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
	- temp : temperature in Celsius
	- atemp: feeling temperature in Celsius
	- hum: humidity
	- windspeed: wind speed
	- casual: count of casual users
	- registered: count of registered users
	- cnt: count of total rental bikes including both casual and registered
"""

data = pd.read_csv("day.csv")
data.head()

data.tail()

data.shape

data.info()

data.describe().T

data.isnull().sum()

# check for duplicates

data_duplicates = data.copy()
data_duplicates.drop_duplicates(subset = None, inplace = True)
data_duplicates.shape

# by using the drop function we understood that there are no duplicates in the data table as both original data and data_duplicates show the same number of rows

#to check for distinct unique values
data.nunique()

for col in data.columns:
    print(data[col].value_counts(dropna = False).sort_index(ascending = True), '\n\n\n')

data.columns

"""## Removing columns based on data dictionary and business understanding

In the dataset provided, you will notice that there are three columns named 'casual', 'registered', and 'cnt'. The variable 'casual' indicates the number casual users who have made a rental. The variable 'registered' on the other hand shows the total number of registered users who have made a booking on a given day. Finally, the 'cnt' variable indicates the total number of bike rentals, including both casual and registered. The model should be built taking this 'cnt' as the target variable.

The 1st column 'instant' is more similar to index column.

Also, in the dataset we have 'yr' and 'mnth' as separate columns and column 'dteday' is repeating the same information .

Hence, we can remove 'instant' , 'dteday', 'casual' and  'registered' columns which can create problems while selecting best features for model building.
"""

data_new = data[['season', 'yr', 'mnth', 'holiday', 'weekday',
       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',
       'cnt']]

data_new.head(3)

# since 'cnt' is my TARGET(dependet variable)
data_new = data_new[['cnt','season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday',
       'weathersit', 'temp', 'atemp', 'hum', 'windspeed']]

data_new.head(2)

# let us create dummy variables

data_new['yr'] = data_new['yr'].astype('category')
data_new['mnth'] = data_new['mnth'].astype('category')
data_new['weekday'] = data_new['weekday'].astype('category')
data_new['workingday'] = data_new['workingday'].astype('category')
data_new['season'] = data_new['season'].astype('category')
data_new['weathersit'] = data_new['weathersit'].astype('category')
data_new['holiday'] = data_new['holiday'].astype('category')

data_new.info()

plt.figure(figsize = (25,10))
plt.subplot(2,3,1)
sns.boxplot(x= 'season', y = 'cnt', data = data_new)
plt.subplot(2,3,2)
sns.boxplot(x= 'mnth', y = 'cnt', data = data_new)
plt.subplot(2,3,3)
sns.boxplot(x= 'weathersit', y = 'cnt', data = data_new)
plt.subplot(2,3,4)
sns.boxplot(x= 'holiday', y = 'cnt', data = data_new)
plt.subplot(2,3,5)
sns.boxplot(x= 'weekday', y = 'cnt', data = data_new)
plt.subplot(2,3,6)
sns.boxplot(x= 'workingday', y = 'cnt', data = data_new)
plt.show()

"""From the six categorical columns we get the below insights

The inference that we could derive are as below:

season: Almost 32% of the bike booking were happening in season3 with a median of over 5000 booking (for the period of 2 years). This was followed by season2 & season4 with 27% & 25% of total booking. This indicates, season can be a good predictor for the dependent variable.

mnth: Almost 10% of the bike booking were happening in the months 5,6,7,8 & 9 with a median of over 4000 booking per month. This indicates, mnth has some trend for bookings and can be a good predictor for the dependent variable.

weathersit: Almost 67% of the bike booking were happening during ‘weathersit1 with a median of close to 5000 booking (for the period of 2 years). This was followed by weathersit2 with 30% of total booking. This indicates, weathersit does show some trend towards the bike bookings can be a good predictor for the dependent variable.

holiday: Almost 97.6% of the bike booking were happening when it is not a holiday which means this data is clearly biased. This indicates, holiday CANNOT be a good predictor for the dependent variable.

weekday: weekday variable shows very close trend (between 13.5%-14.8% of total booking on all days of the week) having their independent medians between 4000 to 5000 bookings. This variable can have some or no influence towards the predictor. I will let the model decide if this needs to be added or not.

workingday: Almost 69% of the bike booking were happening in ‘workingday’ with a median of close to 5000 booking (for the period of 2 years). This indicates, workingday can be a good predictor for the dependent variable
"""

data_new = pd.get_dummies(data_new)

data_new.info()

bool_columns = data_new.select_dtypes(include = 'uint8').columns
data_new[bool_columns] = data_new[bool_columns].astype(int)
data_new.head().T

data_new.shape

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

np.random.seed(0)
data_new_train, data_new_test = train_test_split(data_new,train_size = 0.8, random_state = 100)

print(data_new_train.shape)
print(data_new_test.shape)

data_num = data_new_train[['hum','temp', 'atemp', 'windspeed','cnt']]
sns.pairplot(data_num, diag_kind = 'kde')
plt.show()

"""#'cnt', 'temp', 'atemp', 'hum', 'windspeed', 'season_1', 'season_2',
       'season_3', 'season_4', 'yr_0', 'yr_1', 'mnth_1', 'mnth_2', 'mnth_3',
       'mnth_4', 'mnth_5', 'mnth_6', 'mnth_7', 'mnth_8', 'mnth_9', 'mnth_10',
       'mnth_11', 'mnth_12', 'holiday_0', 'holiday_1', 'weekday_0',
       'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5',
       'weekday_6', 'workingday_0', 'workingday_1', 'weathersit_1',
       'weathersit_2', 'weathersit_3'
"""

plt.figure(figsize = (25,20))
sns.heatmap(data_new.corr(), annot = True, cmap = 'YlGnBu')
plt.show()

data_new.columns

scaler = MinMaxScaler()
num_vars = ['hum','temp', 'atemp', 'windspeed','cnt']
data_new_train[num_vars] = scaler.fit_transform(data_new_train[num_vars])
data_new_train.head()

data_new_train.describe().T

X_train = data_new_train
y_train = data_new_train.pop('cnt')

X_train.head()

y_train.head()

lr = LinearRegression()
lr.fit(X_train, y_train)

rfe = RFE(estimator=lr, n_features_to_select=20)
rfe = rfe.fit(X_train, y_train)

selected_features = X_train.columns[rfe.support_]
print("Selected Features:", selected_features)

list(zip(X_train.columns,rfe.support_,rfe.ranking_))

col = X_train.columns[rfe.support_]
col

X_train.columns[~rfe.support_]

X_train_rfe = X_train[col]

#Model 1 :
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = pd.DataFrame()
vif['Features'] = X_train_rfe.columns
vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by="VIF", ascending=False)

vif

"""Building Linear Model using 'STATS MODEL'

Model 1: VIF check


A VIF of 1 indicates no correlation between the variable and other predictors.
A VIF between 1 and 5 indicates moderate correlation.
A VIF greater than 5 indicates high correlation, and anything above 10 is considered very high, suggesting serious multicollinearity.
"""

import statsmodels.api as sm
X_train_lr = sm.add_constant(X_train_rfe)
lr = sm.OLS(y_train,X_train_lr).fit()

lr.params

print(lr.summary())

# Model 2